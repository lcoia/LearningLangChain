{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPcc4oq1oyH5vws7Wm9vbW/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcoia/LearningLangChain/blob/main/Chapter1/Chapter1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XTKyJabSO0C",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-groq langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "a-llm.py\n",
        "Note: using a free Groq model instead of paid OpenAI\n",
        "\n",
        "https://medium.com/data-engineer-things/bigquerys-ridiculous-pricing-model-cost-us-10-000-in-just-22-seconds-7d52e3e4ae60\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_groq.chat_models import ChatGroq"
      ],
      "metadata": {
        "id": "YzkMMr7TTbQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store your API keys in Google Colab Secrets\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "A24ZZp2MdcAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGroq(model=\"llama3-70b-8192\", api_key=userdata.get('GROQ_API_KEY'))"
      ],
      "metadata": {
        "id": "i7-dFN6RT1Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(\"The sky is\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "KAbBgBWFhjI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "b-chat.py\n",
        "\n",
        "HumanMessage - A message sent from the perspective of the human, with user role.\n",
        "\"\"\"\n",
        "from langchain_core.messages import HumanMessage\n",
        "prompt = [HumanMessage(\"What is the capital of France?\")]"
      ],
      "metadata": {
        "id": "iV90uSwBiL8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "2jyAxflaiv4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "c-system.py\n",
        "\n",
        "SystemMessage - A message setting the instructions the AI should follow, with the system role.\n",
        "\"\"\"\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "system_msg = SystemMessage(\n",
        "    \"You are a helpful assistant that responds to questions with three exclamation marks.\"\n",
        ")\n",
        "human_msg = HumanMessage(\"What is the capital of France?\")\n",
        "\n",
        "response = model.invoke([system_msg, human_msg])\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "tF8aRVwri2Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "d-promt.py\n",
        "\n",
        "PromptTemplate - Making LLM prompts reusable\n",
        "\n",
        "https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/\n",
        "\"\"\"\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below.\n",
        "If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\")\n",
        "\n",
        "prompt = template.invoke(\n",
        "    {\n",
        "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
        "        \"question\": \"Which model providers offer LLMs?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "0JRS98FhjJ3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03cc5e6d-20de-467b-90c7-b81ca3a1378c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='Answer the question based on the context below.\\nIf the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face\\'s `transformers` library, or by utilizing OpenAI and Cohere\\'s offerings through the `openai` and `cohere` libraries, respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "e-prompt-model.py\n",
        "\n",
        "Invoke the model with the prompt\n",
        "\"\"\"\n",
        "response = model.invoke(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmPpAGEfg3nu",
        "outputId": "75087cfe-0519-4a5b-b1b5-c8d38b9fa657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='OpenAI and Cohere.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 136, 'total_tokens': 143, 'completion_time': 0.031434802, 'prompt_time': 0.003932116, 'queue_time': 0.241867012, 'total_time': 0.035366918}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-0a83a7fd-3923-437f-9acf-f16307ae6d46-0' usage_metadata={'input_tokens': 136, 'output_tokens': 7, 'total_tokens': 143}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "f-chat-prompt.py\n",
        "\n",
        "ChatPromptTemplate - Prompt template for chat models.\n",
        "Note: This example uses the\n",
        "\"\"\"\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
        "        ),\n",
        "        (\"human\", \"Context: {context}\"),\n",
        "        (\"human\", \"Question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = template.invoke(\n",
        "    {\n",
        "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
        "        \"question\": \"Which model providers offer LLMs?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tejLW5ZRixhw",
        "outputId": "110c798a-7432-4b31-a65e-8cec136e3400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "g-chat-prompt-model.py\n",
        "\"\"\"\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
        "        ),\n",
        "        (\"human\", \"Context: {context}\"),\n",
        "        (\"human\", \"Question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "prompt = template.invoke(\n",
        "    {\n",
        "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
        "        \"question\": \"Which model providers offer LLMs?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(model.invoke(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNgdrSJWrIh1",
        "outputId": "d77fdba1-ed85-4c31-e011-c20a2c140b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='According to the context, the model providers that offer LLMs are OpenAI and Cohere.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 147, 'total_tokens': 168, 'completion_time': 0.06, 'prompt_time': 0.004462331, 'queue_time': 0.242037065, 'total_time': 0.064462331}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-b58d224a-0186-4be5-bb25-10f4c1d38686-0' usage_metadata={'input_tokens': 147, 'output_tokens': 21, 'total_tokens': 168}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "h-structured.py\n",
        "\n",
        "Getting specific output formats from the model.\n",
        "\"\"\"\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
        "\n",
        "    answer: str\n",
        "    \"\"\"The answer to the user's question\"\"\"\n",
        "    justification: str\n",
        "    \"\"\"Justification for the answer\"\"\"\n",
        "\n",
        "structured_llm = model.with_structured_output(AnswerWithJustification)\n",
        "response = structured_llm.invoke(\n",
        "    \"What weighs more, a pound of bricks or a pound of feathers\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "u9NjTjMAYj_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "https://python.langchain.com/docs/how_to/structured_output/#pydantic-class\n",
        "\n",
        "Beyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring,\n",
        "and the names and provided descriptions of parameters are very important.\n",
        "Most of the time with_structured_output is using a model's function/tool calling API,\n",
        "and you can effectively think of all of this information as being added to the model prompt.\n",
        "\"\"\"\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Pydantic\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "    rating: Optional[int] = Field(\n",
        "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
        "    )\n",
        "\n",
        "structured_llm = model.with_structured_output(Joke)\n",
        "response = structured_llm.invoke(\"Tell me a joke about cats\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqwhsoXVZpal",
        "outputId": "797df955-2216-4497-b5d6-df2187c3a6df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setup='Why did the cat join a band?' punchline='Because it wanted to be the purr-cussionist!' rating=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "i-csv.py\n",
        "\n",
        "https://python.langchain.com/api_reference/core/output_parsers.html\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "response = parser.invoke(\"apple, banana, cherry\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH2v6QcwcaUn",
        "outputId": "7ce2d8c5-c284-4f58-d5f9-bbfb96d39163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple', 'banana', 'cherry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke() takes a single input and returns a single output.\n",
        "\n",
        "completion = model.invoke(\"What is the capital of France?\")\n",
        "print(completion)\n"
      ],
      "metadata": {
        "id": "Xn5kQ2KUnsZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch() takes a list of inputs and returns a list of outputs.\n",
        "\n",
        "completions = model.batch([\"What is the capital of Ohio?\", \"What is the capital of Spain?\"])\n",
        "print(completions)"
      ],
      "metadata": {
        "id": "YAkVs3rWodbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stream() takes a single input and returns an iterator of parts of the output as they become available.\n",
        "\n",
        "for token in model.stream(\"What is the capital of Germany?\"):\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "IIsz5nc1ofs4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}