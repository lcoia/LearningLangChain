{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/8ESXqrScbpCCSyh3a0Uu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcoia/LearningLangChain/blob/main/Chapter1/Chapter1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XTKyJabSO0C",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-groq langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "a-llm.py\n",
        "Note: using a free Groq model instead of paid OpenAI\n",
        "\n",
        "https://medium.com/data-engineer-things/bigquerys-ridiculous-pricing-model-cost-us-10-000-in-just-22-seconds-7d52e3e4ae60\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_groq.chat_models import ChatGroq"
      ],
      "metadata": {
        "id": "YzkMMr7TTbQI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store your API keys in Google Colab Secrets\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "A24ZZp2MdcAw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGroq(model=\"llama3-70b-8192\", api_key=userdata.get('GROQ_API_KEY'))"
      ],
      "metadata": {
        "id": "i7-dFN6RT1Nl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(\"The sky is\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "KAbBgBWFhjI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "b-chat.py\n",
        "\n",
        "HumanMessage - A message sent from the perspective of the human, with user role.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "prompt = [HumanMessage(\"What is the capital of France?\")]"
      ],
      "metadata": {
        "id": "iV90uSwBiL8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "2jyAxflaiv4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c06aaea-a6b4-429d-ccd9-ca23cea11a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "c-system.py\n",
        "\n",
        "SystemMessage - A message setting the instructions the AI should follow, with the system role.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "system_msg = SystemMessage(\n",
        "    \"You are a helpful assistant that responds to questions with three exclamation marks.\"\n",
        ")\n",
        "human_msg = HumanMessage(\"What is the capital of France?\")\n",
        "\n",
        "response = model.invoke([system_msg, human_msg])\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "tF8aRVwri2Lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1ec844-359e-437f-f171-2b4bd11b309c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "d-promt.py\n",
        "\n",
        "PromptTemplate - Making LLM prompts reusable\n",
        "\n",
        "https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below.\n",
        "If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\")\n",
        "\n",
        "prompt = template.invoke(\n",
        "    {\n",
        "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
        "        \"question\": \"Which model providers offer LLMs?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "0JRS98FhjJ3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7a199b-643e-4130-e1af-96ccf306d323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='Answer the question based on the context below.\\nIf the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face\\'s `transformers` library, or by utilizing OpenAI and Cohere\\'s offerings through the `openai` and `cohere` libraries, respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "e-prompt-model.py\n",
        "\n",
        "Invoke the model with the prompt\n",
        "\"\"\"\n",
        "\n",
        "response = model.invoke(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "MmPpAGEfg3nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282fa58d-2ea7-4646-a295-b7ae95d380d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='OpenAI and Cohere.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 135, 'total_tokens': 142, 'completion_time': 0.031155378, 'prompt_time': 0.004619241, 'queue_time': 0.22522169, 'total_time': 0.035774619}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run--645c4169-8779-4d66-a4b4-5a1eb2d13d41-0' usage_metadata={'input_tokens': 135, 'output_tokens': 7, 'total_tokens': 142}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "f-chat-prompt.py\n",
        "\n",
        "ChatPromptTemplate - Prompt template for chat models.\n",
        "\n",
        "Make the static prompt from the previous example reusable.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
        "        ),\n",
        "        (\"human\", \"Context: {context}\"),\n",
        "        (\"human\", \"Question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = template.invoke(\n",
        "    {\n",
        "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
        "        \"question\": \"Which model providers offer LLMs?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tejLW5ZRixhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f28763-bc71-46c3-bec3-b6ef916ff029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "g-chat-prompt-model.py\n",
        "\n",
        "Invoke the model with the prompt\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
        "        ),\n",
        "        (\"human\", \"Context: {context}\"),\n",
        "        (\"human\", \"Question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "prompt = template.invoke(\n",
        "    {\n",
        "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
        "        \"question\": \"Which model providers offer LLMs?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(model.invoke(prompt))"
      ],
      "metadata": {
        "id": "BNgdrSJWrIh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc26c3d-7a56-47a8-b466-001bdfa4d2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='According to the context, OpenAI and Cohere offer LLMs.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 147, 'total_tokens': 163, 'completion_time': 0.067725801, 'prompt_time': 0.004328744, 'queue_time': 0.23005148, 'total_time': 0.072054545}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run--b2a5fab6-3bad-4ee2-9433-4ed3fb9db45c-0' usage_metadata={'input_tokens': 147, 'output_tokens': 16, 'total_tokens': 163}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "h-structured.py\n",
        "\n",
        "Getting specific output formats from the model.\n",
        "\n",
        "https://python.langchain.com/docs/how_to/structured_output/\n",
        "\n",
        "Note:\n",
        "This example is suppoed to return JSON ouput, but it actually returns a pydantic object.\n",
        "\n",
        "with_structured_output()\n",
        "  Takes a schema as input which specifies the names, types, and descriptions of the desired output attributes.\n",
        "  If a Pydantic class is used then a Pydantic object will be returned.\n",
        "\n",
        "\n",
        "https://python.langchain.com/docs/concepts/output_parsers/\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
        "\n",
        "    answer: str\n",
        "    \"\"\"The answer to the user's question\"\"\"\n",
        "    justification: str\n",
        "    \"\"\"Justification for the answer\"\"\"\n",
        "\n",
        "structured_llm = model.with_structured_output(AnswerWithJustification)\n",
        "response = structured_llm.invoke(\n",
        "    \"What weighs more, a pound of bricks or a pound of feathers\")\n",
        "print(type(response))\n",
        "print(response)"
      ],
      "metadata": {
        "id": "u9NjTjMAYj_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d03e6e7-86e9-49cd-b38d-5942f9c4fa77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.AnswerWithJustification'>\n",
            "answer='They weigh the same' justification=\"One pound is one pound, regardless of the object's density or size.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "How to return structured data from a model\n",
        "\n",
        "https://python.langchain.com/docs/how_to/structured_output/\n",
        "\n",
        "\n",
        "Typed Dictionary\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "\n",
        "# TypedDict\n",
        "class Joke(TypedDict):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
        "\n",
        "    # Alternatively, we could have specified setup as:\n",
        "\n",
        "    # setup: str                    # no default, no description\n",
        "    # setup: Annotated[str, ...]    # no default, no description\n",
        "    # setup: Annotated[str, \"foo\"]  # default, no description\n",
        "\n",
        "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
        "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
        "\n",
        "\n",
        "structured_llm = model.with_structured_output(Joke)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnmZSdOr9ZXo",
        "outputId": "fb7a6afd-761b-4674-e0ad-27edc105c4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'setup': 'Why did the cat join a band?',\n",
              " 'punchline': 'Because it wanted to be the purr-cussionist!'}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "How to return structured data from a model\n",
        "\n",
        "https://python.langchain.com/docs/how_to/structured_output/\n",
        "\n",
        "JSON Schema\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "json_schema = {\n",
        "    \"title\": \"joke\",\n",
        "    \"description\": \"Joke to tell user.\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"setup\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The setup of the joke\",\n",
        "        },\n",
        "        \"punchline\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The punchline to the joke\",\n",
        "        },\n",
        "        \"rating\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
        "            \"default\": None,\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"setup\", \"punchline\"],\n",
        "}\n",
        "structured_llm = model.with_structured_output(json_schema)\n",
        "\n",
        "structured_llm.invoke(\"Tell me a joke about cats\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSzibu-XcHyp",
        "outputId": "eec81100-38db-41e7-afab-6b233d13f022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'setup': 'Why did the cat join a band?',\n",
              " 'punchline': 'Because it wanted to be the purr-cussionist!'}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "https://python.langchain.com/docs/how_to/structured_output/#pydantic-class\n",
        "\n",
        "Beyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring,\n",
        "and the names and provided descriptions of parameters are very important.\n",
        "Most of the time with_structured_output is using a model's function/tool calling API,\n",
        "and you can effectively think of all of this information as being added to the model prompt.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Pydantic\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "    rating: Optional[int] = Field(\n",
        "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
        "    )\n",
        "\n",
        "structured_llm = model.with_structured_output(Joke)\n",
        "response = structured_llm.invoke(\"Tell me a joke about cats\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "pqwhsoXVZpal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e644b0-1dd5-4319-954f-34e627d4920b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setup='Why did the cat join a band?' punchline='Because it wanted to be the purr-cussionist!' rating=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "How to return structured data from a model\n",
        "\n",
        "https://python.langchain.com/docs/how_to/structured_output/\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Define your desired data structure.\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"question to set up a joke\")\n",
        "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
        "\n",
        "\n",
        "# And a query intented to prompt a language model to populate the data structure.\n",
        "joke_query = \"Tell me a joke.\"\n",
        "\n",
        "# Set up a parser + inject instructions into the prompt template.\n",
        "parser = JsonOutputParser(pydantic_object=Joke)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "chain = prompt | model | parser\n",
        "\n",
        "response = chain.invoke({\"query\": joke_query})\n",
        "print(type(response))\n",
        "print(response)\n",
        "parser.get_format_instructions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "qepp3J7obrYu",
        "outputId": "3a598ad2-0f6a-4a35-c386-b6f460db1ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "{'setup': \"Why couldn't the bicycle stand up by itself?\", 'punchline': 'Because it was two-tired.'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"description\": \"question to set up a joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"answer to resolve the joke\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "i-csv.py\n",
        "\n",
        "https://python.langchain.com/api_reference/core/output_parsers.html\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "response = parser.invoke(\"apple, banana, cherry\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "jH2v6QcwcaUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a857590-9cd1-48d8-ec69-c122af01abb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple', 'banana', 'cherry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# invoke() takes a single input and returns a single output.\n",
        "\n",
        "completion = model.invoke(\"What is the capital of France?\")\n",
        "print(completion)\n"
      ],
      "metadata": {
        "id": "Xn5kQ2KUnsZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c1f804-c8ec-4f35-c6f7-9d9b919a7793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The capital of France is Paris!' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 17, 'total_tokens': 25, 'completion_time': 0.027074323, 'prompt_time': 0.000258957, 'queue_time': 0.229761114, 'total_time': 0.02733328}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run--c8bf0a46-5252-464f-88ca-4eb5b6c643cc-0' usage_metadata={'input_tokens': 17, 'output_tokens': 8, 'total_tokens': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch() takes a list of inputs and returns a list of outputs.\n",
        "\n",
        "completions = model.batch([\"What is the capital of Ohio?\", \"What is the capital of Spain?\"])\n",
        "print(completions)"
      ],
      "metadata": {
        "id": "YAkVs3rWodbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72fe0783-92f6-4cb0-c087-dbdcdfb121e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AIMessage(content='The capital of Ohio is Columbus.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 17, 'total_tokens': 25, 'completion_time': 0.022857143, 'prompt_time': 0.000245298, 'queue_time': 0.229564453, 'total_time': 0.023102441}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--9b9cb914-4359-4829-97dd-9993775a16e1-0', usage_metadata={'input_tokens': 17, 'output_tokens': 8, 'total_tokens': 25}), AIMessage(content='The capital of Spain is Madrid.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 17, 'total_tokens': 25, 'completion_time': 0.022857143, 'prompt_time': 0.000456706, 'queue_time': 0.217671048, 'total_time': 0.023313849}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run--44e4c1ca-faf2-4058-8f46-61baa01c4f1d-0', usage_metadata={'input_tokens': 17, 'output_tokens': 8, 'total_tokens': 25})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stream() takes a single input and returns an iterator of parts of the output as they become available.\n",
        "\n",
        "for token in model.stream(\"What is the capital of Germany?\"):\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "IIsz5nc1ofs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "k-imperative.py\n",
        "\n",
        "Imperative Composition\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "# the building blocks\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# combine them in a function\n",
        "# chain decorator adds the same Runnable interface for any function you write\n",
        "\n",
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    return model.invoke(prompt)\n",
        "\n",
        "# use it\n",
        "\n",
        "response = chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "sIbPTQ0FTOaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ka-stream.py\n",
        "\n",
        "Streaming\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    for token in model.stream(prompt):\n",
        "        yield token\n",
        "\n",
        "\n",
        "for part in chatbot.stream({\"question\": \"Which model providers offer LLMs?\"}):\n",
        "    print(part)"
      ],
      "metadata": {
        "id": "8ru3ubhRTz6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "i-declarative.py\n",
        "\n",
        "Declarative Composition\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    for token in model.stream(prompt):\n",
        "        yield token\n",
        "\n",
        "\n",
        "for part in chatbot.stream({\"question\": \"Which model providers offer LLMs?\"}):\n",
        "    print(part)"
      ],
      "metadata": {
        "id": "zXZa3s1WUQqE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}