{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOtq5CXXKGgkgNhcvYfk0Gr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcoia/LearningLangChain/blob/main/Chapter1/Chapter1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XTKyJabSO0C",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-groq langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a-llm.py\n",
        "# Note: using a free Groq model instead of paid OpenAI\n",
        "from langchain_groq.chat_models import ChatGroq"
      ],
      "metadata": {
        "id": "YzkMMr7TTbQI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store your API keys in Google Colab Secrets\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "A24ZZp2MdcAw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "I'm using Grog (https://groq.com/) because it provides a free api tier.\n",
        "https://console.groq.com/docs/rate-limits\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "model = ChatGroq(model=\"llama3-70b-8192\", api_key=userdata.get('GROQ_API_KEY'))"
      ],
      "metadata": {
        "id": "i7-dFN6RT1Nl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(\"The sky is\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "KAbBgBWFhjI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eca0f5e-06fc-4332-9424-ef2140c6ac07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...blue!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "b-chat.py\n",
        "\n",
        "HumanMessage - A message sent from the perspective of the human, with user role.\n",
        "\"\"\"\n",
        "from langchain_core.messages import HumanMessage\n",
        "prompt = [HumanMessage(\"What is the capital of France?\")]"
      ],
      "metadata": {
        "id": "iV90uSwBiL8k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "2jyAxflaiv4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d58fd43a-8a92-4e72-c3e6-4309db7ce239"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's an easy one! The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "c-system.py\n",
        "\n",
        "SystemMessage - A message setting the instructions the AI should follow, with the system role.\n",
        "\"\"\"\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "system_msg = SystemMessage(\n",
        "    \"You are a helpful assistant that responds to questions with three exclamation marks.\"\n",
        ")\n",
        "human_msg = HumanMessage(\"What is the capital of France?\")\n",
        "\n",
        "response = model.invoke([system_msg, human_msg])\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "tF8aRVwri2Lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e768c29c-babc-43d1-e106-9a77659e0b75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "d-promt.py\n",
        "\n",
        "PromptTemplate - Making LLM prompts reusable\n",
        "\n",
        "https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/\n",
        "\"\"\"\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below.\n",
        "If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\")\n",
        "\n",
        "prompt = template.invoke(\n",
        "    {\n",
        "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
        "        \"question\": \"Which model providers offer LLMs?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "0JRS98FhjJ3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4502638a-41a0-4b1f-8a1e-2aa3f65790f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text='Answer the question based on the context below.\\nIf the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face\\'s `transformers` library, or by utilizing OpenAI and Cohere\\'s offerings through the `openai` and `cohere` libraries, respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "e-prompt-model.py\n",
        "\n",
        "Invoke the model with the prompt\n",
        "\"\"\"\n",
        "response = model.invoke(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmPpAGEfg3nu",
        "outputId": "ce3e91df-8075-46bd-b142-c260e5e69f12"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='OpenAI and Cohere.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 135, 'total_tokens': 142, 'completion_time': 0.031282396, 'prompt_time': 0.004219737, 'queue_time': 0.219664299, 'total_time': 0.035502133}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-b397f682-2b29-49a8-9e51-fa6e1b2ae111-0' usage_metadata={'input_tokens': 135, 'output_tokens': 7, 'total_tokens': 142}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "f-chat-prompt.py\n",
        "\n",
        "ChatPromptTemplate - Prompt template for chat models.\n",
        "Note: This example uses the\n",
        "\"\"\"\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
        "        ),\n",
        "        (\"human\", \"Context: {context}\"),\n",
        "        (\"human\", \"Question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "response = template.invoke(\n",
        "    {\n",
        "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
        "        \"question\": \"Which model providers offer LLMs?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tejLW5ZRixhw",
        "outputId": "08a0ad5d-9c27-4a38-c52f-720c2786fcae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "g-chat-prompt-model.py\n",
        "\"\"\"\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
        "        ),\n",
        "        (\"human\", \"Context: {context}\"),\n",
        "        (\"human\", \"Question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "prompt = template.invoke(\n",
        "    {\n",
        "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
        "        \"question\": \"Which model providers offer LLMs?\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(model.invoke(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNgdrSJWrIh1",
        "outputId": "f5240fa6-e961-4ae4-b0b0-868d913bc755"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='According to the context, OpenAI and Cohere offer Large Language Models (LLMs) through their respective libraries, `openai` and `cohere`.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 147, 'total_tokens': 180, 'completion_time': 0.094285714, 'prompt_time': 0.004637223, 'queue_time': 0.219722153, 'total_time': 0.098922937}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-328626d4-4157-47bc-8c50-0f6632f7d173-0' usage_metadata={'input_tokens': 147, 'output_tokens': 33, 'total_tokens': 180}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "h-structured.py\n",
        "\n",
        "Getting specific output formats from the model.\n",
        "\"\"\"\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "    \"\"\"An answer to the user's question along with justification for the answer.\"\"\"\n",
        "\n",
        "    answer: str\n",
        "    \"\"\"The answer to the user's question\"\"\"\n",
        "    justification: str\n",
        "    \"\"\"Justification for the answer\"\"\"\n",
        "\n",
        "structured_llm = model.with_structured_output(AnswerWithJustification)\n",
        "response = structured_llm.invoke(\n",
        "    \"What weighs more, a pound of bricks or a pound of feathers\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "u9NjTjMAYj_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55080404-58f6-41c4-d7de-c284fdaa0daa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "answer='They weigh the same' justification='A pound is a unit of weight or mass, so one pound of bricks and one pound of feathers both weigh the same amount, one pound.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "https://python.langchain.com/docs/how_to/structured_output/#pydantic-class\n",
        "\n",
        "Beyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring,\n",
        "and the names and provided descriptions of parameters are very important.\n",
        "Most of the time with_structured_output is using a model's function/tool calling API,\n",
        "and you can effectively think of all of this information as being added to the model prompt.\n",
        "\"\"\"\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Pydantic\n",
        "class Joke(BaseModel):\n",
        "    \"\"\"Joke to tell user.\"\"\"\n",
        "\n",
        "    setup: str = Field(description=\"The setup of the joke\")\n",
        "    punchline: str = Field(description=\"The punchline to the joke\")\n",
        "    rating: Optional[int] = Field(\n",
        "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
        "    )\n",
        "\n",
        "structured_llm = model.with_structured_output(Joke)\n",
        "response = structured_llm.invoke(\"Tell me a joke about cats\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqwhsoXVZpal",
        "outputId": "47bad0cb-c585-421b-b5e4-46a0a921381e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "setup='Why did the cat join a band?' punchline='Because it wanted to be the purr-cussionist!' rating=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "i-csv.py\n",
        "\n",
        "https://python.langchain.com/api_reference/core/output_parsers.html\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "response = parser.invoke(\"apple, banana, cherry\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH2v6QcwcaUn",
        "outputId": "8d915536-da25-4690-b879-d48239f325d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple', 'banana', 'cherry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "j-methods.py\n",
        "\n",
        "invoke() takes a single input and returns a single output.\n",
        "\"\"\"\n",
        "\n",
        "completion = model.invoke(\"What is the capital of France?\")\n",
        "print(completion)\n"
      ],
      "metadata": {
        "id": "Xn5kQ2KUnsZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb64b01-be52-48dd-c1c0-b745001c240c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The capital of France is Paris.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 17, 'total_tokens': 25, 'completion_time': 0.022857143, 'prompt_time': 0.000247397, 'queue_time': 0.21881393899999999, 'total_time': 0.02310454}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None} id='run-3695c0df-c633-4f27-8ee6-7d953f8841df-0' usage_metadata={'input_tokens': 17, 'output_tokens': 8, 'total_tokens': 25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch() takes a list of inputs and returns a list of outputs.\n",
        "\n",
        "completions = model.batch([\"What is the capital of Ohio?\", \"What is the capital of Spain?\"])\n",
        "print(completions)"
      ],
      "metadata": {
        "id": "YAkVs3rWodbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11729280-9f54-4f1d-ca73-7598b17f1504"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AIMessage(content='The capital of Ohio is Columbus.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 17, 'total_tokens': 25, 'completion_time': 0.022857143, 'prompt_time': 0.000238987, 'queue_time': 0.221827048, 'total_time': 0.02309613}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run-52eec8a0-0af0-4d09-8b4d-4fd844a35f11-0', usage_metadata={'input_tokens': 17, 'output_tokens': 8, 'total_tokens': 25}), AIMessage(content='The capital of Spain is Madrid.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 17, 'total_tokens': 25, 'completion_time': 0.022857143, 'prompt_time': 0.000240067, 'queue_time': 0.21761535099999998, 'total_time': 0.02309721}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'finish_reason': 'stop', 'logprobs': None}, id='run-6b8c2825-084f-4e9f-acf3-504c41f8ab7b-0', usage_metadata={'input_tokens': 17, 'output_tokens': 8, 'total_tokens': 25})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stream() takes a single input and returns an iterator of parts of the output as they become available.\n",
        "\n",
        "for token in model.stream(\"What is the capital of Germany?\"):\n",
        "    print(token)"
      ],
      "metadata": {
        "id": "IIsz5nc1ofs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "k-imperative.py\n",
        "\n",
        "Imperative Composition\n",
        " @chain decorator addes the same Runnable interface for any function you write.\n",
        " It adds to invoke(), batch(), and stream() methods\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatGroq(model=\"llama3-70b-8192\",api_key=userdata.get('GROQ_API_KEY'))\n",
        "\n",
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    return model.invoke(prompt)\n",
        "\n",
        "response = chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "_4jTcZTRKjFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ka-stream.py\n",
        "\n",
        "Enable streaming or async support.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@chain\n",
        "def chatbot(values):\n",
        "    prompt = template.invoke(values)\n",
        "    for token in model.stream(prompt):\n",
        "        yield token\n",
        "\n",
        "\n",
        "for part in chatbot.stream({\"question\": \"Which model providers offer LLMs?\"}):\n",
        "    print(part)\n"
      ],
      "metadata": {
        "id": "aausA3oTQfBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "kb-async.py\n",
        "\n",
        "Asynchronous execution\n",
        "\n",
        "Note: won't execute in Colab\n",
        "\"\"\"\n",
        "\n",
        "@chain\n",
        "async def chatbot(values):\n",
        "    prompt = await template.ainvoke(values)\n",
        "    return await model.ainvoke(prompt)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    return await chatbot.ainvoke({\"question\": \"Which model providers offer LLMs?\"})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    print(asyncio.run(main()))"
      ],
      "metadata": {
        "id": "HfIlEZkvRPkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "i-declarative.py\n",
        "\n",
        "Declarative Composition\n",
        "\n",
        "LangChain Expression Language (LCEL) is a declarative language for composing LangChain components.\n",
        "LangChain compiles LCEL compositions into an optimzed execution plan, with\n",
        "automatic parallelization, streaming, tracing, and async support.\n",
        "\"\"\"\n",
        "\n",
        "chatbot = template | model\n",
        "\n",
        "# use it\n",
        "\n",
        "response = chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "ngbGYWX9R4Ow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}