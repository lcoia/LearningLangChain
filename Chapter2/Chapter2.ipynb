{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCv+KAxW4GEQ53Y2FQtemM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcoia/LearningLangChain/blob/main/Chapter2/Chapter2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZaKstluTsML"
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain-google-genai langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "a-text-loader.py\n",
        "\n",
        "Note: Files are not saved after the Colab runtime terminates.\n",
        "Load the sample text file, ECB_policymakers.txt, from the sample_data folder.\n",
        "Please upload the text file, ECB_policymakers.txt, from the link below to the sample_data folder.\n",
        "\n",
        "https://drive.google.com/file/d/1pO-DRfmc5KuHIZbD75hffcyAEmICclbL/view?usp=sharing\n",
        "\n",
        "\n",
        "LangChain Loaders\n",
        "https://python.langchain.com/api_reference/community/document_loaders.html\n",
        "\n",
        "\n",
        "Vector Similarity\n",
        "https://www.pinecone.io/learn/vector-similarity/\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader('./sample_data/ECB_policymakers.txt', encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "a5f5khQa1vKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Beautiful Soup is a library that makes it easy to scrape information from web pages.\n",
        "This library is required for the next example.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "%pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "KAE1UZgZ93D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "b-web-loader.py\n",
        "\n",
        "Load a web page.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader('https://www.langchain.com/')\n",
        "docs = loader.load()\n",
        "\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "xwOTUpwf9b8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pypdf is a PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files.\n",
        "This library is required for the next example.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "%pip install pypdf"
      ],
      "metadata": {
        "id": "neSkUp2uDEAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "c-pdf-loader.py\n",
        "\n",
        "Note: Files are not saved after the Colab runtime terminates.\n",
        "Please upload the PDF file from the link below to the sample_data folder.\n",
        "\n",
        "https://www.babson.edu/media/babson/assets/cutler-center/Introduciton-to-Technical-Analysis.pdf\n",
        "\n",
        "\"\"\"\n",
        "import pprint\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader('./sample_data/Introduciton-to-Technical-Analysis.pdf')\n",
        "pages = loader.load()\n",
        "\n",
        "pprint.pprint(pages)"
      ],
      "metadata": {
        "id": "w-WKwu3v-SsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "d-rec-text-splitter.py\n",
        "\n",
        "Split the document into chunks to fit in the context window of the LLM.\n",
        "Keep semantically related chunks together.\n",
        "\n",
        "LangChain Text Splitters\n",
        "https://python.langchain.com/docs/concepts/text_splitters/\n",
        "\n",
        "\n",
        "Late chunking for better semantic context.\n",
        "https://www.datacamp.com/tutorial/late-chunking\n",
        "https://docs.chonkie.ai/chunkers/overview\n",
        "\n",
        "MTEB Embedding Models (Massive Text Embedding Benchmark)\n",
        "https://modal.com/blog/mteb-leaderboard-article\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "import pprint\n",
        "\n",
        "loader = TextLoader('./sample_data/ECB_policymakers.txt', encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splitted_docs = splitter.split_documents(docs)\n",
        "\n",
        "pprint.pprint(splitted_docs)\n"
      ],
      "metadata": {
        "id": "hsZ1alqvmi1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "e-rec-text-splitter-code.py\n",
        "\n",
        "Split code languages and Markdown into semantic chunks.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_text_splitters import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "\n",
        "PYTHON_CODE = \"\"\" def hello_world(): print(\"Hello, World!\") # Call the function hello_world() \"\"\"\n",
        "\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
        ")\n",
        "\n",
        "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
        "\n",
        "print(python_docs)"
      ],
      "metadata": {
        "id": "XMhr_743cdpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "g-embeddings.py\n",
        "\n",
        "Generating text embeddings.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: Google model names must be prefixed with 'models/'\n",
        "model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "embeddings = model.embed_documents([\n",
        "    \"Hi there!\",\n",
        "    \"Oh, hello!\",\n",
        "    \"What's your name?\",\n",
        "    \"My friends call me World\",\n",
        "    \"Hello World!\"\n",
        "])\n",
        "\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "TLE9eiTIdHu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "h-load-split-embed.py\n",
        "\n",
        "Load, split, and generate embeddings.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the document\n",
        "loader = TextLoader(\"./sample_data/ECB_policymakers.txt\", encoding=\"utf-8\")\n",
        "doc = loader.load()\n",
        "\n",
        "# Split the document\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(doc)\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "embeddings = embeddings_model.embed_documents(\n",
        "    [chunk.page_content for chunk in chunks]\n",
        ")\n",
        "\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "OuA1r_7crluc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Substituting Qdrant for Postgres as a vector store.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "!pip install langchain-qdrant\n"
      ],
      "metadata": {
        "id": "ZIbVF0Q0RDIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "j-record-manager.py\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from langchain.indexes import SQLRecordManager, index\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "client = QdrantClient(\":memory:\")\n",
        "client.create_collection(\n",
        "    collection_name=\"demo_collection\",\n",
        "    vectors_config=Vectorparams(size=3072, distance=Distance.COSINE))\n",
        "\n",
        "vectorstore = QdrantVectorStore(\n",
        "    embeddings=embeddings_model,\n",
        "    collection_name=\"demo_collection\",\n",
        "    client=client,\n",
        ")\n",
        "\n",
        "record_manager = SQLRecordManager(\n",
        "    namespace,\n",
        "    db_url=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\",\n",
        ")\n",
        "\n",
        "# Create the schema if it doesn't exist\n",
        "record_manager.create_schema()\n",
        "\n",
        "# Create documents\n",
        "docs = [\n",
        "    Document(page_content='there are cats in the pond', metadata={\n",
        "             \"id\": 1, \"source\": \"cats.txt\"}),\n",
        "    Document(page_content='ducks are also found in the pond', metadata={\n",
        "             \"id\": 2, \"source\": \"ducks.txt\"}),\n",
        "]\n",
        "\n",
        "# Index the documents\n",
        "index_1 = index(\n",
        "    docs,\n",
        "    record_manager,\n",
        "    vectorstore,\n",
        "    cleanup=\"incremental\",  # prevent duplicate documents\n",
        "    source_id_key=\"source\",  # use the source field as the source_id\n",
        ")\n",
        "\n",
        "print(\"Index attempt 1:\", index_1)\n",
        "\n",
        "# second time you attempt to index, it will not add the documents again\n",
        "index_2 = index(\n",
        "    docs,\n",
        "    record_manager,\n",
        "    vectorstore,\n",
        "    cleanup=\"incremental\",\n",
        "    source_id_key=\"source\",\n",
        ")\n",
        "\n",
        "print(\"Index attempt 2:\", index_2)\n",
        "\n",
        "# If we mutate a document, the new version will be written and all old versions sharing the same source will be deleted.\n",
        "\n",
        "docs[0].page_content = \"I just modified this document!\"\n",
        "\n",
        "index_3 = index(\n",
        "    docs,\n",
        "    record_manager,\n",
        "    vectorstore,\n",
        "    cleanup=\"incremental\",\n",
        "    source_id_key=\"source\",\n",
        ")\n",
        "\n",
        "print(\"Index attempt 3:\", index_3)\n"
      ],
      "metadata": {
        "id": "IPluMyAyD7u7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264db3b1-c6c7-49a8-eef7-1443f17fafad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f5c894f73c28>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcollection_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"my_docs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0membeddings_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-embedding-3-small\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"my_docs_namespace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/embeddings/base.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_proxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0msync_specific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msync_specific\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai_proxy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_async_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ]
    }
  ]
}